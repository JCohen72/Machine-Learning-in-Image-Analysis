{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SIGMA_SQUARED_01 = 1.0    # Variance of prior (0/1)\n",
    "EPSILON_01 = 0.01         # Learning rate (0/1)\n",
    "\n",
    "SIGMA_SQUARED_68 = 1.0     # Variance of prior (6/8)\n",
    "EPSILON_68 = 0.01          # Learning rate (6/8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_data(folder, digits):\n",
    "    \"\"\"\n",
    "    Load MNIST data from text files and filter for specific digits.\n",
    "\n",
    "    Parameters:\n",
    "    - folder (str): Path to the folder containing MNIST data files.\n",
    "    - digits (list): List of digits to filter (e.g., [0, 1]).\n",
    "\n",
    "    Returns:\n",
    "    - X_train (np.ndarray): Training data features.\n",
    "    - y_train (np.ndarray): Training data labels.\n",
    "    - X_test (np.ndarray): Test data features.\n",
    "    - y_test (np.ndarray): Test data labels.\n",
    "    \"\"\"\n",
    "    # Construct file paths\n",
    "    trainX_path = f'{folder}/trainX.txt'\n",
    "    trainY_path = f'{folder}/trainY.txt'\n",
    "    testX_path = f'{folder}/testX.txt'\n",
    "    testY_path = f'{folder}/testY.txt'\n",
    "\n",
    "    # Load data from text files with the correct delimiter\n",
    "    X_train = np.loadtxt(trainX_path, delimiter=',')\n",
    "    y_train = np.loadtxt(trainY_path, delimiter=',').astype(int)\n",
    "    X_test = np.loadtxt(testX_path, delimiter=',')\n",
    "    y_test = np.loadtxt(testY_path, delimiter=',').astype(int)\n",
    "\n",
    "    # Filter for specified digits\n",
    "    train_filter = np.isin(y_train, digits)\n",
    "    test_filter = np.isin(y_test, digits)\n",
    "    X_train = X_train[train_filter]\n",
    "    y_train = y_train[train_filter]\n",
    "    X_test = X_test[test_filter]\n",
    "    y_test = y_test[test_filter]\n",
    "\n",
    "    # Adjust labels to 0 and 1\n",
    "    y_train = np.where(y_train == digits[0], 0, 1)\n",
    "    y_test = np.where(y_test == digits[0], 0, 1)\n",
    "\n",
    "    # Add bias term (column of ones)\n",
    "    X_train = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n",
    "    X_test = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function.\n",
    "\n",
    "    Parameters:\n",
    "    - z (np.ndarray): Input array.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Sigmoid of the input.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, beta, sigma_squared):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss function U(beta).\n",
    "\n",
    "    Parameters:\n",
    "    - X (np.ndarray): Feature matrix.\n",
    "    - y (np.ndarray): Labels vector.\n",
    "    - beta (np.ndarray): Coefficient vector.\n",
    "    - sigma_squared (float): Variance of the prior distribution.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Gradient vector.\n",
    "    \"\"\"\n",
    "    predictions = sigmoid(X @ beta)\n",
    "    error = predictions - y\n",
    "    gradient = X.T @ error + beta / sigma_squared\n",
    "    return gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X, y, beta, sigma_squared):\n",
    "    \"\"\"\n",
    "    Compute the loss function U(beta).\n",
    "\n",
    "    Parameters:\n",
    "    - X (np.ndarray): Feature matrix.\n",
    "    - y (np.ndarray): Labels vector.\n",
    "    - beta (np.ndarray): Coefficient vector.\n",
    "    - sigma_squared (float): Variance of the prior distribution.\n",
    "\n",
    "    Returns:\n",
    "    - float: Loss value.\n",
    "    \"\"\"\n",
    "    predictions = sigmoid(X @ beta)\n",
    "    # Avoid log(0) by adding a small epsilon\n",
    "    epsilon = 1e-15\n",
    "    likelihood = -np.sum(y * np.log(predictions + epsilon) + (1 - y) * np.log(1 - predictions + epsilon))\n",
    "    prior = (1 / (2 * sigma_squared)) * np.sum(beta ** 2)\n",
    "    loss = likelihood + prior\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_logistic_regression(X, y, sigma_squared, epsilon, max_iter=5000, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Perform MAP estimation for logistic regression using gradient descent.\n",
    "\n",
    "    Parameters:\n",
    "    - X (np.ndarray): Feature matrix.\n",
    "    - y (np.ndarray): Labels vector.\n",
    "    - sigma_squared (float): Variance of the prior distribution.\n",
    "    - epsilon (float): Learning rate (step size).\n",
    "    - max_iter (int): Maximum number of iterations.\n",
    "    - tol (float): Tolerance for convergence.\n",
    "\n",
    "    Returns:\n",
    "    - beta (np.ndarray): Estimated coefficients.\n",
    "    \"\"\"\n",
    "    beta = np.zeros(X.shape[1])\n",
    "    for iteration in range(max_iter):\n",
    "        gradient = compute_gradient(X, y, beta, sigma_squared)\n",
    "        beta_new = beta - epsilon * gradient\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(beta_new - beta, ord=1) < tol:\n",
    "            loss = compute_loss(X, y, beta, sigma_squared)\n",
    "            print(f'Converged after {iteration + 1} iterations, Loss: {loss:.4f}.')\n",
    "            break\n",
    "\n",
    "        beta = beta_new\n",
    "\n",
    "    return beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, beta):\n",
    "    \"\"\"\n",
    "    Predict class labels for given data and coefficients.\n",
    "\n",
    "    Parameters:\n",
    "    - X (np.ndarray): Feature matrix.\n",
    "    - beta (np.ndarray): Coefficient vector.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Predicted labels (0 or 1).\n",
    "    \"\"\"\n",
    "    probabilities = sigmoid(X @ beta)\n",
    "    return (probabilities >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Evaluate the model using zero-one loss (average error rate).\n",
    "\n",
    "    Parameters:\n",
    "    - y_true (np.ndarray): True labels.\n",
    "    - y_pred (np.ndarray): Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "    - float: Average error rate.\n",
    "    \"\"\"\n",
    "    error_rate = np.mean(np.abs(y_true - y_pred))\n",
    "    return error_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(folder, digits, sigma_squared, epsilon):\n",
    "    \"\"\"\n",
    "    Run the MAP logistic regression experiment for specified digits.\n",
    "\n",
    "    Parameters:\n",
    "    - folder (str): Path to the folder containing MNIST data files.\n",
    "    - digits (list): Digits to classify (e.g., [0, 1]).\n",
    "    - sigma_squared (float): Variance of the prior distribution.\n",
    "    - epsilon (float): Learning rate (step size).\n",
    "    \"\"\"\n",
    "    print(f'\\nRunning experiment for digits {digits[0]} and {digits[1]}')\n",
    "\n",
    "    # Load and preprocess data\n",
    "    X_train, y_train, X_test, y_test = load_mnist_data(folder, digits)\n",
    "\n",
    "    # Train the model using MAP estimation\n",
    "    beta = map_logistic_regression(X_train, y_train, sigma_squared, epsilon)\n",
    "\n",
    "    # Predict on test data\n",
    "    y_pred = predict(X_test, beta)\n",
    "\n",
    "    # Evaluate the model\n",
    "    error_rate = evaluate_model(y_test, y_pred)\n",
    "    print(f'Average Error Rate (Zero-One Loss): {error_rate:.4f}')\n",
    "    print(f'Parameters used: sigma_squared={sigma_squared}, epsilon={epsilon}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiment for digits 0 and 1\n",
      "Converged after 935 iterations, Loss: 2.0343.\n",
      "Average Error Rate (Zero-One Loss): 0.0000\n",
      "Parameters used: sigma_squared=1.0, epsilon=0.01\n",
      "\n",
      "Running experiment for digits 6 and 8\n",
      "Converged after 992 iterations, Loss: 5.5752.\n",
      "Average Error Rate (Zero-One Loss): 0.0179\n",
      "Parameters used: sigma_squared=1.0, epsilon=0.01\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Path to the data folder\n",
    "    data_folder = 'data-mnist'\n",
    "\n",
    "    # Experiment for digits 0 and 1\n",
    "    run_experiment(folder=data_folder, digits=[0, 1], sigma_squared=SIGMA_SQUARED_01, epsilon=EPSILON_01)\n",
    "\n",
    "    # Experiment for digits 6 and 8\n",
    "    run_experiment(folder=data_folder, digits=[6, 8], sigma_squared=SIGMA_SQUARED_68, epsilon=EPSILON_68)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

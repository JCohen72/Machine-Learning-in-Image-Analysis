{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SIGMA_SQUARED = 1.0  # Variance of prior\n",
    "EPSILON = 0.01       # Learning rate\n",
    "N_COMPONENTS_LIST = [10, 20, 30] # List of numbers of principal components to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_data(folder, digits):\n",
    "    \"\"\"\n",
    "    Load MNIST data from text files and filter for specific digits.\n",
    "\n",
    "    Parameters:\n",
    "    - folder (str): Path to the folder containing MNIST data files.\n",
    "    - digits (list): List of digits to filter (e.g., [0, 1]).\n",
    "\n",
    "    Returns:\n",
    "    - X_train (np.ndarray): Training data features.\n",
    "    - y_train (np.ndarray): Training data labels.\n",
    "    - X_test (np.ndarray): Test data features.\n",
    "    - y_test (np.ndarray): Test data labels.\n",
    "    \"\"\"\n",
    "    # Construct file paths\n",
    "    trainX_path = f'{folder}/trainX.txt'\n",
    "    trainY_path = f'{folder}/trainY.txt'\n",
    "    testX_path = f'{folder}/testX.txt'\n",
    "    testY_path = f'{folder}/testY.txt'\n",
    "\n",
    "    # Load data from text files with the correct delimiter\n",
    "    X_train = np.loadtxt(trainX_path, delimiter=',')\n",
    "    y_train = np.loadtxt(trainY_path, delimiter=',').astype(int)\n",
    "    X_test = np.loadtxt(testX_path, delimiter=',')\n",
    "    y_test = np.loadtxt(testY_path, delimiter=',').astype(int)\n",
    "\n",
    "    # Filter for specified digits\n",
    "    train_filter = np.isin(y_train, digits)\n",
    "    test_filter = np.isin(y_test, digits)\n",
    "    X_train = X_train[train_filter]\n",
    "    y_train = y_train[train_filter]\n",
    "    X_test = X_test[test_filter]\n",
    "    y_test = y_test[test_filter]\n",
    "\n",
    "    # Adjust labels to 0 and 1\n",
    "    y_train = (y_train == digits[1]).astype(int)\n",
    "    y_test = (y_test == digits[1]).astype(int)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function.\n",
    "\n",
    "    Parameters:\n",
    "    - z (np.ndarray): Input array.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Sigmoid of the input.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, beta, sigma_squared):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss function U(beta).\n",
    "\n",
    "    Parameters:\n",
    "    - X (np.ndarray): Feature matrix.\n",
    "    - y (np.ndarray): Labels vector.\n",
    "    - beta (np.ndarray): Coefficient vector.\n",
    "    - sigma_squared (float): Variance of the prior distribution.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Gradient vector.\n",
    "    \"\"\"\n",
    "    predictions = sigmoid(X @ beta)\n",
    "    error = predictions - y\n",
    "    gradient = X.T @ error + beta / sigma_squared\n",
    "    return gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_logistic_regression(X, y, sigma_squared, epsilon, max_iter=5000, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Perform MAP estimation for logistic regression using gradient descent.\n",
    "\n",
    "    Parameters:\n",
    "    - X (np.ndarray): Feature matrix.\n",
    "    - y (np.ndarray): Labels vector.\n",
    "    - sigma_squared (float): Variance of the prior distribution.\n",
    "    - epsilon (float): Learning rate (step size).\n",
    "    - max_iter (int): Maximum number of iterations.\n",
    "    - tol (float): Tolerance for convergence.\n",
    "\n",
    "    Returns:\n",
    "    - beta (np.ndarray): Estimated coefficients.\n",
    "    \"\"\"\n",
    "    beta = np.zeros(X.shape[1])\n",
    "    for iteration in range(max_iter):\n",
    "        gradient = compute_gradient(X, y, beta, sigma_squared)\n",
    "        beta_new = beta - epsilon * gradient\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(beta_new - beta, ord=1) < tol:\n",
    "            print(f'Converged after {iteration + 1} iterations.')\n",
    "            break\n",
    "\n",
    "        beta = beta_new\n",
    "\n",
    "    return beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, beta):\n",
    "    \"\"\"\n",
    "    Predict class labels for given data and coefficients.\n",
    "\n",
    "    Parameters:\n",
    "    - X (np.ndarray): Feature matrix.\n",
    "    - beta (np.ndarray): Coefficient vector.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Predicted labels (0 or 1).\n",
    "    \"\"\"\n",
    "    probabilities = sigmoid(X @ beta)\n",
    "    return (probabilities >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Evaluate the model using zero-one loss (average error rate).\n",
    "\n",
    "    Parameters:\n",
    "    - y_true (np.ndarray): True labels.\n",
    "    - y_pred (np.ndarray): Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "    - float: Average error rate.\n",
    "    \"\"\"\n",
    "    error_rate = np.mean(np.abs(y_true - y_pred))\n",
    "    return error_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pca_experiment(folder, digits, sigma_squared, epsilon, n_components_list):\n",
    "    \"\"\"\n",
    "    Run the PCA and MAP logistic regression experiments for specified digits.\n",
    "\n",
    "    Parameters:\n",
    "    - folder (str): Path to the folder containing MNIST data files.\n",
    "    - digits (list): Digits to classify (e.g., [0, 1]).\n",
    "    - sigma_squared (float): Variance of the prior distribution.\n",
    "    - epsilon (float): Learning rate (step size).\n",
    "    - n_components_list (list): List of numbers of principal components to use.\n",
    "    \"\"\"\n",
    "    # Load and preprocess data\n",
    "    X_train_full, y_train, X_test_full, y_test = load_mnist_data(folder, digits)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for n_components in n_components_list:\n",
    "        print(f'\\nRunning experiment with {n_components} principal components.')\n",
    "\n",
    "        # Record the start time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Apply PCA to training data\n",
    "        pca = PCA(n_components=n_components)\n",
    "        X_train_reduced = pca.fit_transform(X_train_full)\n",
    "\n",
    "        # Apply the same PCA transformation to test data\n",
    "        X_test_reduced = pca.transform(X_test_full)\n",
    "\n",
    "        # Add bias term to reduced data\n",
    "        X_train = np.hstack([np.ones((X_train_reduced.shape[0], 1)), X_train_reduced])\n",
    "        X_test = np.hstack([np.ones((X_test_reduced.shape[0], 1)), X_test_reduced])\n",
    "\n",
    "        # Train the model using MAP estimation\n",
    "        beta = map_logistic_regression(X_train, y_train, sigma_squared, epsilon)\n",
    "\n",
    "        # Predict on test data\n",
    "        y_pred = predict(X_test, beta)\n",
    "\n",
    "        # Evaluate the model\n",
    "        error_rate = evaluate_model(y_test, y_pred)\n",
    "        accuracy = 1 - error_rate\n",
    "\n",
    "        # Record the end time\n",
    "        end_time = time.time()\n",
    "        time_consumption = end_time - start_time\n",
    "\n",
    "        # Print results\n",
    "        print(f'Number of Principal Components: {n_components}')\n",
    "        print(f'Accuracy: {accuracy:.4f}')\n",
    "        print(f'Time Consumption: {time_consumption:.4f} seconds')\n",
    "        print(f'Parameters used: sigma_squared={sigma_squared:.1f}, epsilon={epsilon:.2f}')\n",
    "\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            'n_components': n_components,\n",
    "            'accuracy': accuracy,\n",
    "            'time_consumption': time_consumption\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiment with 10 principal components.\n",
      "Converged after 615 iterations.\n",
      "Number of Principal Components: 10\n",
      "Accuracy: 1.0000\n",
      "Time Consumption: 0.0350 seconds\n",
      "Parameters used: sigma_squared=1.0, epsilon=0.01\n",
      "\n",
      "Running experiment with 20 principal components.\n",
      "Converged after 698 iterations.\n",
      "Number of Principal Components: 20\n",
      "Accuracy: 1.0000\n",
      "Time Consumption: 0.0295 seconds\n",
      "Parameters used: sigma_squared=1.0, epsilon=0.01\n",
      "\n",
      "Running experiment with 30 principal components.\n",
      "Converged after 754 iterations.\n",
      "Number of Principal Components: 30\n",
      "Accuracy: 1.0000\n",
      "Time Consumption: 0.0405 seconds\n",
      "Parameters used: sigma_squared=1.0, epsilon=0.01\n",
      "\n",
      "Summary of Results:\n",
      "PC = 10, Accuracy = 1.0000, Time = 0.0350 seconds, Parameters used: sigma_squared=1.0, epsilon=0.01\n",
      "PC = 20, Accuracy = 1.0000, Time = 0.0295 seconds, Parameters used: sigma_squared=1.0, epsilon=0.01\n",
      "PC = 30, Accuracy = 1.0000, Time = 0.0405 seconds, Parameters used: sigma_squared=1.0, epsilon=0.01\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Path to the data folder\n",
    "    data_folder = 'data-mnist'\n",
    "\n",
    "    # Run the PCA experiments\n",
    "    results = run_pca_experiment(\n",
    "        folder=data_folder,\n",
    "        digits=[0, 1],\n",
    "        sigma_squared=SIGMA_SQUARED,\n",
    "        epsilon=EPSILON,\n",
    "        n_components_list=N_COMPONENTS_LIST\n",
    "    )\n",
    "\n",
    "    # Print a summary of results\n",
    "    print('\\nSummary of Results:')\n",
    "    for res in results:\n",
    "        print(f\"PC = {res['n_components']}, Accuracy = {res['accuracy']:.4f}, Time = {res['time_consumption']:.4f} seconds, Parameters used: sigma_squared={sigma_squared:.1f}, epsilon={epsilon:.2f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
